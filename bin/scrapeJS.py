from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
import time
import pandas as pd
'''
------------------------------------------------------------
Set up webdriver (browser)
------------------------------------------------------------
'''
#--Set options
#Device capability
def setupDcaps():
    dcaps = DesiredCapabilities.PHANTOMJS
    dcaps['phantomjs.page.settings.loadImages'] = False
    dcaps['phantomjs.page.settings.userAgent'] = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37'
    return dcaps
dcaps = setupDcaps()

#Additional driver options
DOWNLOAD_TIMEOUT = 20
LOG_PATH = 'log/ghostdriver.log'








'''
------------------------------------------------------------
Browse and acquire html - restaurant main page
------------------------------------------------------------
'''

#%%
def collectMainPage(shopId):
    #--Initiate browser (refresh the browser)
    #Replace with .Firefox(), or with the browser of choice (options could be different)
    browser = webdriver.PhantomJS(
        desired_capabilities=dcaps,
        service_log_path=LOG_PATH)
    browser.set_page_load_timeout(DOWNLOAD_TIMEOUT)


    #--Targeting a main page url and navigate to that page
    url = 'http://www.dianping.com/shop/' + str(shopId)
    browser.get(url)


    #--Acquire general user rating generated by JS
    #Actions
    webdriver.ActionChains(browser
        ).click(on_element=browser.find_element_by_css_selector('.brief-info > a.icon')
        ).perform()

    #Get HTML
    HTML_generalScore = browser.execute_script('return document.getElementById("shop-score").innerHTML')


    #--Acquire recommended dishes generated by JS
        #Actions
    try:
        webdriver.ActionChains(browser
            ).click(on_element=browser.find_element_by_css_selector('.recommend-name + .J-more')
            ).perform()

        #Get HTML
        HTML_recDish = browser.execute_script('return document.getElementsByClassName("recommend-name")[0].innerHTML')
    except: HTML_recDish = None


    #--Acquire main page + basic info generated by JS
    #Actions
    webdriver.ActionChains(browser
        ).click(on_element=browser.find_element_by_css_selector('.basic-info > a.J-unfold')
        ).perform()

    #Get HTML
    HTML_main = browser.execute_script('return document.documentElement.outerHTML')


    #--Return the scraped contents
    return HTML_generalScore, HTML_recDish, HTML_main



#%%
HTML_generalScores = []
HTML_recDishes = []

OUTPUT_PATH = '../data/'
shopIds = [93230555, 90404278, 75010650, 6088238, 22303139]
for shopId in shopIds:
    print(shopId)
    HTML_generalScore, HTML_recDish, HTML_main = collectMainPage(shopId)
    
    with open(OUTPUT_PATH + 'raw/' + str(shopId) + '.html', 'w+', encoding='utf-8') as f:
        f.write(HTML_main)

    HTML_generalScores.append(HTML_generalScore)
    HTML_recDishes.append(HTML_recDish)

    #--Set a timeout between each request
    time.sleep(3)


df_shopInfo_HTML = pd.DataFrame({
    'shopIds'           : shopIds,
    'HTML_generalScores': HTML_generalScores,
    'HTML_recDishes'    : HTML_recDishes
})

#%%
df_shopInfo_HTML.to_csv(OUTPUT_PATH + 'df_shopInfo_HTML.csv', encoding='utf-8')








'''
------------------------------------------------------------
Extract info from HTML - restaurant main page
------------------------------------------------------------
'''
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import re




'''
From soup
'''
soupCauldron = []
def makeSoups(fldr):
    for filename in os.listdir(fldr):
        content = open(fldr + filename, 'r', errors='replace', encoding='utf-8')
        soup = BeautifulSoup(content.read(), 'html5lib')
        yield (soup, filename)

soupCauldron = makeSoups(OUTPUT_PATH + 'raw/')


#%%
shopIds = [93230555, 90404278, 75010650, 6088238, 22303139]
content = open(OUTPUT_PATH + 'raw/' + str(shopIds[3]) + '.html', 'r', errors='replace', encoding='utf-8')
soup = BeautifulSoup(content.read(), 'html5lib')


#%%
#Shop name
try: shopName = re.sub('\n\n.*', '', soup.h1.get_text()).strip('\n ')
except: shopName = None

#Category
try: shopCat = 
re.sub('[\n ]', '', soup.find(class_='breadcrumb').get_text())
except: shopCat = None

#Number of Branches
try: branchNum = re.search('([0-9]+)家分店', soup.h1.get_text()).group(1)
except: branchNum = None

#Number of reviews
try: reviewNum = soup.find(id='reviewCount').get_text().rstrip('条评论')
except: reviewNum = None

#Average consumption
try: avgConsume = soup.find(id='avgPriceTitle').get_text().strip('人均：元')
except: avgConsume = None

#Address
try: address = soup.find(class_='expand-info address').find(itemprop='street-address')['title']
except: address = None

#Tel Number
try: tel = soup.find(class_='expand-info tel').find(itemprop='tel').get_text()
except: tel = None

#Special tags (團、訂、外、促)
try:
    specialTags = str(soup.find(class_='promosearch-wrapper'))
    tag_tuan = 'tag-tuan' in specialTags
    tag_ding = 'tag-ding' in specialTags
    tag_wai = 'tag-wai' in specialTags
    tag_cu = 'tag-cu' in specialTags
except:
    tag_tuan = None
    tag_ding = None
    tag_wai = None
    tag_cu = None

#Extra info (operation hour, simple desc, parking, alias, crowd-sourced)
try: extraInfo = soup.find(class_='other J-other').get_text()
re.sub('\n+', '\n', extraInfo.strip('\n ')))
except: extraInfo = None




'''
From extra df
'''
df_shopInfo_HTML = pd.read_csv(OUTPUT_PATH + 'df_shopInfo_HTML.csv')


df_shopInfo_HTML.HTML_generalScores[0]
soup_scores = BeautifulSoup(df_shopInfo_HTML.HTML_generalScores[0], 'html5lib')

#5 4 3 2 1 stars
star = re.findall('\d+', soup_scores.find(class_='stars').get_text())

#口味 環境 服務
score = re.findall('\d+\.\d+', soup_scores.find(class_='scores').get_text())


df_shopInfo_HTML.HTML_recDishes[0]
soup_dishes = BeautifulSoup(df_shopInfo_HTML.HTML_recDishes[0], 'html5lib')

soup_dishes.find(class_='recommend-name').get_text()