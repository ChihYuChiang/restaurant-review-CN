from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
import pandas as pd
import time
import os








'''
------------------------------------------------------------
Set up webdriver (browser)
------------------------------------------------------------
'''
#--Acquire updated user agents from: https://techblog.willshouse.com/2012/01/03/most-common-user-agents/
userAgentCandidates = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8'
]


#--Set options
#Device capability
def setupDcaps():
    dcaps = DesiredCapabilities.PHANTOMJS
    dcaps['phantomjs.page.settings.loadImages'] = False

    #Randomly acquire 0 to 4
    i = int(((time.time() % 1 * 10) // 1) // 2)
    
    #Randomly assign user agent from candidates
    dcaps['phantomjs.page.settings.userAgent'] = userAgentCandidates[0]

    return dcaps

#Additional driver options
DOWNLOAD_TIMEOUT = 10
LOG_PATH = 'log/ghostdriver.log'








'''
------------------------------------------------------------
Browse and acquire html - restaurant review pages
------------------------------------------------------------
'''
import numpy as np
import csv

LOG_PATH = 'modules/log/ghostdriver.log'
shopId = 9951252

url = 'http://www.dianping.com/shop/' + str(shopId)

browser = webdriver.PhantomJS(
    desired_capabilities=setupDcaps(),
    service_log_path=LOG_PATH)
browser.set_page_load_timeout(DOWNLOAD_TIMEOUT)

browser.get(url)
content0 = browser.execute_script('return document.documentElement.outerHTML')


time.sleep(2)
webdriver.ActionChains(browser
        ).click(on_element=browser.find_element_by_css_selector('p.comment-all > a')
        ).perform()
content1 = browser.execute_script('return document.documentElement.outerHTML')

browser.quit()


for page in np.arange(1, 3):
    print(page)

    browser = webdriver.PhantomJS(
        desired_capabilities=setupDcaps(),
        service_log_path=LOG_PATH)
    browser.set_page_load_timeout(DOWNLOAD_TIMEOUT)

    url =     'http://www.dianping.com/shop/6088238/review_more?pageno=' + str(page)
    browser.get(url)



    # content = browser.execute_script('return document.documentElement.outerHTML')
    # print(content)

    with open('../data/raw/review/' + str(shopId) + '.csv', 'a', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([content])
    
    browser.quit()

    #Set a timeout between each request
    time.sleep(2)


reviews = pd.read_csv('../data/raw/review/' + str(shopId) + '.csv', index_col=False, header=None)
reviews.shape
type(reviews)
reviews[0][0]








'''
------------------------------------------------------------
Browse and acquire html - restaurant main page
------------------------------------------------------------
'''
#%%
def mainPage(shopId, OUTPUT_PATH):
    #--Initiate browser (refresh the browser)
    #Replace with .Firefox(), or with the browser of choice (options could be different)
    browser = webdriver.PhantomJS(
        desired_capabilities=setupDcaps(),
        service_log_path=LOG_PATH)
    browser.set_page_load_timeout(DOWNLOAD_TIMEOUT)


    #--Targeting a main page url and navigate to that page
    url = 'http://www.dianping.com/shop/' + str(shopId)
    browser.get(url)


    #--Acquire general user rating generated by JS
    #Actions
    webdriver.ActionChains(browser
        ).click(on_element=browser.find_element_by_css_selector('.brief-info > a.icon')
        ).perform()

    #Get HTML
    HTML_generalScore = browser.execute_script('return document.getElementById("shop-score").innerHTML')


    #--Acquire recommended dishes generated by JS
    #Actions
    try:
        webdriver.ActionChains(browser
            ).click(on_element=browser.find_element_by_css_selector('.recommend-name + .J-more')
            ).perform()

    #Get HTML
        HTML_recDish = browser.execute_script('return document.getElementsByClassName("shop-tab-recommend")[0].innerHTML')
    except: HTML_recDish = None


    #--Acquire main page + basic info generated by JS
    #Actions
    webdriver.ActionChains(browser
        ).click(on_element=browser.find_element_by_css_selector('.basic-info > a.J-unfold')
        ).perform()

    #Get HTML
    HTML_main = browser.execute_script('return document.documentElement.outerHTML')


    #--Close browser
    browser.quit()


    #--Save info to file
    #Write additional info into 1 single csv
    #Append to csv (when file doesn't exist, include header as well)
    entry_extraInfo_HTML = pd.DataFrame({
        'shopId'            : [shopId],
        'HTML_generalScores': [HTML_generalScore],
        'HTML_recDishes'    : [HTML_recDish]
    })

    OUTPUT_FILE = OUTPUT_PATH + 'raw/df_extraInfo_HTML.csv'
    entry_extraInfo_HTML.to_csv(OUTPUT_FILE, header=not os.path.exists(OUTPUT_FILE), index=False, encoding='utf-8', mode='a')

    #Save main page, 1 restaurant per file
    with open(OUTPUT_PATH + 'raw/main' + str(shopId) + '.html', 'w+', encoding='utf-8') as f:
        f.write(HTML_main)